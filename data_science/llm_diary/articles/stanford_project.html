<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>stanford_project</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="/assets/css/style_colour.css"></link>
  <link rel="icon" type="image/x-icon" href="/assets/icons/SEC.ico?">
</head>
<body>
<nav id="TOC">
<ul>
<li><a href="#a-project-i-wrote---machine-learning-for-llm-toxicity">A project I wrote - Machine Learning for LLM Toxicity</a><ul>
<li><a href="#the-project">The Project</a></li>
<li><a href="#the-experience">The Experience</a></li>
</ul></li>
</ul>
</nav>
<h1 id="a-project-i-wrote---machine-learning-for-llm-toxicity">A project I wrote - Machine Learning for LLM Toxicity</h1>
<p>Something I’ve worked and studied a lot is how to try and combat toxic behaviour from LLMs. What was previously hilarious - <a href="https://www.bbc.co.uk/news/technology-68025677">watching chatbots getting jailbroken</a> - has now become a fear that one day it could be my fault that this has happened…</p>
<p>I’ll write more about toxicity classification at some point and its history - for now I wanted to write a bit about my experience of research.</p>
<h2 id="the-project">The Project</h2>
<p>Here are some links to my research project, literature review and experimental protocol.</p>
<ul>
<li><a href="/assets/pdfs/CS224u_final_papers.pdf">Project</a></li>
<li><a href="/assets/pdfs/CS224u_experiment_protocol.pdf">Experimental Protocol</a></li>
<li><a href="/assets/pdfs/CS224u_lit_review.pdf">Literature Review</a></li>
</ul>
<p>The key hypothesis was to see whether you could try and use model unlearning (i.e. what if we just do gradient ascent instead of descent) in order to try and limit the impact that toxic weights have.</p>
<p>The main difference between this and fine-tuning is the use of KL divergence, which is now being introduced into some fine-tuning regimes.</p>
<h2 id="the-experience">The Experience</h2>
<p>This was something done outside of work hours - so trying to do a job and research at the same time was certainly tough. It was good to get the feeling of becoming more well-known in a topic that I felt actually mattered and people cared about, combining my love of all things on the bleeding edge of tech and policy behind that (something you’d expect a maths and philosophy alumni to love).</p>
<p>I had an offer to publish this from someone outside of Stanford, the issue was I wasn’t happy enough with the result and would like to do more research into the topic first. Model Unlearning could be the way forward in a world where Big Tech don’t want to audit their own datasets used for pre-training properly.</p>
<p>A key takeaway is things go wrong - all the time. Another point to make is that hardware limitations are a huge issue for LLM research. You’re basically not able to do anything too crazy without having a number of mid-to-high-end GPUs. Stories emerge at the moment of start-ups struggling with this as well.</p>
<p>Overall, I really enjoyed the experience and I’m mostly proud of what I achieved, as with any good project there’s always the desire to take things further as well! Hopefully I’ll get the chance to.</p>
</body>
</html>
