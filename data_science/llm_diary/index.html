<!DOCTYPE html>
<html>
     <head>
         <title>Data Science</title>
         <link rel="stylesheet" href="/assets/css/style_greyscale.css"></link>
         <meta name="viewport" content="width=device-width,initial-scale=1.0">
     </head>

     <body>
        <header>
            <nav>
                <a href="/">Me</a> |
                <a href="/mathematics/">Maths</a> |
                <a href="/data_science/">Data Science</a> |
                <a href="/football/">Football</a> |
                <a href="/yugioh/">Yu-Gi-Oh</a> |
                <a href="/cooking/">Cooking</a> |
                <a href="/misc/">Other Stuff</a>
            </nav>
        </header>
         <p>Having spent 2024 getting deep into how to build LLM chatbots - it's important to understand the theory. Here are my attempts at explaining some papers I think are important.</p>
         <ul>
            <li><a href="/data_science/llm_diary/articles/emergent_behaviour.html">Emergent Behaviours</a>(15/05)</li>
            <li><a href="/data_science/llm_diary/articles/real_toxicity_prompts.html">REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models (29/02)</a>
            <li><a href="/data_science/llm_diary/articles/LLM-as-a-judge.html">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena (07/02)</a></li>
            <li>I had the pleasure to be part of an LLM workshop at a large tech company. <a href="/data_science/llm_diary/article/LLM_workshop_thoughts.html">Here are some general notes inspired by this time (05-06/02)</a></li>
            <li><a href="/data_science/llm_diary/articles/RAG_survey_P1.html">Retrieval-Augmented Generation for Large Language Models: A Survey (30/01 - Part 1)</a></li>
            <li><a href="/data_science/llm_diary/articles/adaptLLMs.html">Adapting Large Language Models via Reading Comprehension (29/01)</a></li>
            <li><a href="/data_science/llm_diary/articles/retentive_network.html">Retentive Networks (16/01)</a></li>
            <li><a href="/data_science/llm_diary/articles/attention_is_all_you_need.html">Attention is All you Need (16/01)</a></li>
            </ul></href>
     </body>
</html>