<!DOCTYPE html>
<html>
     <head>
         <title>Data Science</title>
         <link rel="stylesheet" href="/assets/css/style_greyscale.css"></link>
         <meta name="viewport" content="width=device-width,initial-scale=1.0">
     </head>

     <body>
        <header>
            <nav>
                <a href="/">Me</a> |
                <a href="/mathematics/">Maths</a> |
                <a href="/data_science/">Data Science</a> |
                <a href="/football/">Football</a> |
                <a href="/yugioh/">Yu-Gi-Oh</a> |
                <a href="/cooking/">Cooking</a> |
                <a href="/misc/">Other Stuff</a>
            </nav>
        </header>
         <p>Having spent 2024 getting deep into how to build LLM chatbots - it's important to understand the theory. Here are my attempts at explaining some papers I think are important.</p>
         <ul>
            <li><href a="\articles\real_toxicity_prompts.md">REALTOXICITYPROMPTS: Evaluating Neural Toxic Degeneration in Language Models (29/02)</href>
            <li></li><href a="\articles\LLM-as-a-judge.md">Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena (07/02)</href>
            <li>I had the pleasure to be part of an LLM workshop at a large tech company. <href a="\article\LLM_workshop_thoughts.md">Here are some general notes inspired by this time (05-06/02)</href></li>
            <li><href a="\articles\RAG_survey_P1.md">Retrieval-Augmented Generation for Large Language Models: A Survey (30/01 - Part 1)</href></li>
            <li><href a="\articles\adaptLLMs.md">Adapting Large Language Models via Reading Comprehension (29/01)</href></li>
            <li><href a="\articles\retentive_network.md">Retentive Networks (16/01)</href></li>
            <li><href a="\articles\attention_is_all_you_need.md">Attention is All you Need (16/01)</li>
            </ul></href>
     </body>
</html>